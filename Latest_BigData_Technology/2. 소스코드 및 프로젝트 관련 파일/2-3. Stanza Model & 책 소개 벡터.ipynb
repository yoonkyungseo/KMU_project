{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2af2797",
   "metadata": {},
   "source": [
    "# Stanza 형태소 분석기로 model 및 문장 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d6326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from datetime import datetime\n",
    "# import chromedriver_autoinstaller\n",
    "\n",
    "from matplotlib import font_manager, rc\n",
    "# 차트에서 한글 출력을 위한 설정\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "your_os = platform.system()\n",
    "if your_os == 'Linux':\n",
    "    rc('font', family='NanumGothic')\n",
    "elif your_os == 'Windows':\n",
    "    ttf = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "    font_name = font_manager.FontProperties(fname=ttf).get_name()\n",
    "    \n",
    "    rc('font', family=font_name)\n",
    "elif your_os == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "rc('axes', unicode_minus=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "487378a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common. by import By\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import konlpy\n",
    "from konlpy.tag import Okt\n",
    "from konlp.kma.klt2023 import klt2023\n",
    "import nltk\n",
    "import stanza\n",
    "from gensim.models import Word2Vec, word2vec\n",
    "from gensim.models import FastText\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2946084b",
   "metadata": {},
   "source": [
    "# 1) Word2Vec, FastText Model 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42341109",
   "metadata": {},
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45de0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거 및 데이터 불러오기\n",
    "df = pd.read_csv('book_all.csv').drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563aebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 책소개에 이모티콘과 문장 부호가 많아, 더 깔끔한 데이터 확보를 위해 제거해줌.\n",
    "for i in range(len(df)):\n",
    "    df['책소개'][i] = re.sub(\"[^A-Za-z0-9가-힣]\",   # 바꿀패턴:영어, 숫자, 한글이 아닌 모든것 제거\n",
    "                      \" \",                   # 바뀐내용:공백으로 바꿔라\n",
    "                      df['책소개'][i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6371d0",
   "metadata": {},
   "source": [
    "### 불용어 처리  \n",
    "- https://deep.chulgil.me/hangugeo-bulyongeo-riseuteu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "629e0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_Stopword = pd.read_csv('한국어 불용어.csv')['불용어'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7829e8",
   "metadata": {},
   "source": [
    "### 형태소 분석기 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2590a46b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc077e0f02743a6bd5bed55743c72c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 02:39:35 INFO: Downloading default packages for language: ko (Korean) ...\n",
      "2023-04-19 02:39:35 INFO: File exists: C:\\Users\\rudtj\\stanza_resources\\ko\\default.zip\n",
      "2023-04-19 02:39:36 INFO: Finished downloading models and saved to C:\\Users\\rudtj\\stanza_resources.\n",
      "2023-04-19 02:39:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3f66f28e5f48638146376f5c29e45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 02:39:37 INFO: Loading these models for language: ko (Korean):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | kaist   |\n",
      "| pos       | kaist   |\n",
      "| lemma     | kaist   |\n",
      "| depparse  | kaist   |\n",
      "=======================\n",
      "\n",
      "2023-04-19 02:39:39 INFO: Using device: cuda\n",
      "2023-04-19 02:39:39 INFO: Loading: tokenize\n",
      "2023-04-19 02:39:43 INFO: Loading: pos\n",
      "2023-04-19 02:39:43 INFO: Loading: lemma\n",
      "2023-04-19 02:39:44 INFO: Loading: depparse\n",
      "2023-04-19 02:39:44 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('ko')\n",
    "nlp = stanza.Pipeline('ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08510909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns_stanza(text):\n",
    "    doc = nlp(text)\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            lemma = word.lemma.split('+')\n",
    "            xpos = word.xpos.split('+')\n",
    "            for lem, pos in zip(lemma, xpos):\n",
    "                if pos.startswith('n'):\n",
    "                    yield lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90568b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=extract_nouns_stanza, stop_words=ko_Stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28740200",
   "metadata": {},
   "source": [
    "해당 텍스트 형태소 분석 - 명사 추출  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "671766b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소요시간 : 0:22:29.184364\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "all_nouns = []\n",
    "for i in range(len(df)):\n",
    "    cv.fit_transform(df.iloc[i,:])\n",
    "    all_nouns.append(cv.get_feature_names())\n",
    "end = datetime.now()\n",
    "print('소요시간 :', end-start)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be5a18b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302814"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "len(list(itertools.chain(*all_nouns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72488cc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_nouns = []\n",
    "for i in all_nouns:\n",
    "    _=[]\n",
    "    for j in i:\n",
    "        if j not in ko_Stopword:\n",
    "            _.append(j)\n",
    "    total_nouns.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c928bf61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302814"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(itertools.chain(*total_nouns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff913899",
   "metadata": {},
   "source": [
    "### word2vec/fasttext model 생성  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67db76",
   "metadata": {},
   "source": [
    "Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1978936",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wv = word2vec.Word2Vec(sentences=total_nouns, vector_size=400, window=5, min_count=1)\n",
    "model_wv.save('Word2Vec_model/word2vec_book_stanza.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02196fc6",
   "metadata": {},
   "source": [
    "FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2688a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = FastText(total_nouns, vector_size=400, window=5, min_count=2)\n",
    "model_ft.save('FastText_model/fasttext_book_stanza.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c704985",
   "metadata": {},
   "source": [
    "# 2) 책 소개 벡터 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "689780e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['책소개_명사'] = all_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb6e3b",
   "metadata": {},
   "source": [
    "불용어 처리 및 중복제거\n",
    "(문장 벡터를 만드는 과정에서 같은 벡터가 여러번 더해지지 않도록 중복 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e5cfa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word = []\n",
    "for i in all_nouns:\n",
    "    _=[]\n",
    "    for j in list(set(i)):\n",
    "        if j not in ko_Stopword:\n",
    "            _.append(j)\n",
    "    total_word.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5b4813e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302814, 302814)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(itertools.chain(*all_nouns))), len(list(itertools.chain(*total_word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24254dde",
   "metadata": {},
   "source": [
    "책 소개별 추출된 명사를 합하여 책소개 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bc1144c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4788\n"
     ]
    }
   ],
   "source": [
    "vec=[]\n",
    "for i in total_word:\n",
    "    v=np.zeros(400)\n",
    "    for k in i:\n",
    "        v += model_ft.wv.get_vector(k)\n",
    "    vec.append(v)\n",
    "print(len(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bc6257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['책소개_벡터'] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53d37994",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('books_vector_stanza.csv',index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
